{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a multi-output model with Keras\n",
    "\n",
    "In this notebook, we will be covering some basics of what is a multi-output model in Keras and how we can take advantage of it when building our Deep Learning models. We will be using the UTK dataset, which contains pictures of people from diverse gender, races and ages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "The UTKFace dataset is a large dataset composed of over 20 thousand face images with their respectivce annotations of age, gender and ethnicity. The images are properly cropped into the face region, but display some variations in pose, illumination, resolution, etc.\n",
    "\n",
    "In order to retrieve the annotations of each record, we need to parse the filenames. Each record is stored in the following format: `[age]_[gender]_[race]_[date&time].jpg`\n",
    "\n",
    "Where:\n",
    "    - age is an integer from 0 to 116\n",
    "    - gender is an integer in which 0 represents male and 1 represents female\n",
    "    - race is an integer from 0 to 4, denoting white, black, asian, indian and others, respectively\n",
    "    - date and time, denoting when the picture was taken\n",
    "    \n",
    "If you want to know more about this dataset, please check their [website](http://aicip.eecs.utk.edu/wiki/UTKFace).\n",
    "\n",
    "\n",
    "Let's initially create a dictionary to help us on parsing the information from the dataset, along with some other information (dataset location, training split, width and height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder_name = 'UTKFace'\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "IM_WIDTH = IM_HEIGHT = 198\n",
    "\n",
    "dataset_dict = {\n",
    "    'race_id': {\n",
    "        0: 'white', \n",
    "        1: 'black', \n",
    "        2: 'asian', \n",
    "        3: 'indian', \n",
    "        4: 'others'\n",
    "    },\n",
    "    'gender_id': {\n",
    "        0: 'male',\n",
    "        1: 'female'\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\n",
    "dataset_dict['race_alias'] = dict((g, i) for i, g in dataset_dict['race_id'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a method to help us on extracting the data from our dataset. This function will be used to iterate over each file of the UTK dataset and return a Pandas Dataframe containing all the fields (age, gender and sex) of our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(dataset_path, ext='jpg'):\n",
    "    \"\"\"\n",
    "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\n",
    "    the data (age, gender and sex) of all files.\n",
    "    \"\"\"\n",
    "    def parse_info_from_file(path):\n",
    "        \"\"\"\n",
    "        Parse information from a single file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.split(path)[1]\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            age, gender, race, _ = filename.split('_')\n",
    "\n",
    "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\n",
    "        except Exception as ex:\n",
    "            return None, None, None\n",
    "        \n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "    \n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "        \n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['age', 'gender', 'race', 'file']\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>C:/Users/leose/first_semester/CS_640/keras-mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>C:/Users/leose/first_semester/CS_640/keras-mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>C:/Users/leose/first_semester/CS_640/keras-mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>C:/Users/leose/first_semester/CS_640/keras-mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>C:/Users/leose/first_semester/CS_640/keras-mul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender   race                                               file\n",
       "0  100.0    male  white  C:/Users/leose/first_semester/CS_640/keras-mul...\n",
       "1  100.0    male  white  C:/Users/leose/first_semester/CS_640/keras-mul...\n",
       "2  100.0  female  white  C:/Users/leose/first_semester/CS_640/keras-mul...\n",
       "3  100.0  female  white  C:/Users/leose/first_semester/CS_640/keras-mul...\n",
       "4  100.0  female  white  C:/Users/leose/first_semester/CS_640/keras-mul..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = parse_dataset('C:/Users/leose/first_semester/CS_640/keras-multi-output-model-utk-face/UTKFace/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "In order to input our data to our Keras multi-output model, we will create a helper object to work as a data generator for our dataset. This will be done by generating batches of data, which will be used to feed our multi-output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11380/3013183918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0mdata_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUtkFaceDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_split_indexes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "\n",
    "class UtkFaceDataGenerator():\n",
    "    \"\"\"\n",
    "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def generate_split_indexes(self):\n",
    "        p = np.random.permutation(len(self.df))\n",
    "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\n",
    "        train_idx = p[:train_up_to]\n",
    "        test_idx = p[train_up_to:]\n",
    "\n",
    "        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n",
    "        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
    "        \n",
    "        # converts alias to id\n",
    "        self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\n",
    "        self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\n",
    "\n",
    "        self.max_age = self.df['age'].max()\n",
    "        \n",
    "        return train_idx, valid_idx, test_idx\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        \"\"\"\n",
    "        Used to perform some minor preprocessing on the image before inputting into the network.\n",
    "        \"\"\"\n",
    "        im = Image.open(img_path)\n",
    "        im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
    "        im = np.array(im) / 255.0\n",
    "        \n",
    "        return im\n",
    "        \n",
    "    def generate_images(self, image_idx, is_training, batch_size=16):\n",
    "        \"\"\"\n",
    "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # arrays to store our batched data\n",
    "        images, ages, races, genders = [], [], [], []\n",
    "        while True:\n",
    "            for idx in image_idx:\n",
    "                person = self.df.iloc[idx]\n",
    "                \n",
    "                age = person['age']\n",
    "                race = person['race_id']\n",
    "                gender = person['gender_id']\n",
    "                file = person['file']\n",
    "                \n",
    "                im = self.preprocess_image(file)\n",
    "                \n",
    "                ages.append(age / self.max_age)\n",
    "                races.append(to_categorical(race, len(dataset_dict['race_id'])))\n",
    "                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\n",
    "                images.append(im)\n",
    "                \n",
    "                # yielding condition\n",
    "                if len(images) >= batch_size:\n",
    "                    yield np.array(images), [np.array(ages), np.array(races), np.array(genders)]\n",
    "                    images, ages, races, genders = [], [], [], []\n",
    "                    \n",
    "            if not is_training:\n",
    "                break\n",
    "                \n",
    "data_generator = UtkFaceDataGenerator(df)\n",
    "train_idx, valid_idx, test_idx = data_generator.generate_split_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model\n",
    "\n",
    "In this step, we will define our multi-output Keras model. Our model will be composed of three major branches, one for each available feature: age, gender and race. The default structure for our convolutional layers is based on a Conv2D layer with a ReLU activation, followed by a BatchNormalization layer, a MaxPooling and finally a Dropout layer. Each of these default layers is then followed by the final layer for each feature, composed by a Dense layer.\n",
    "\n",
    "These default layers are defined on the make_default_hidden_layers method, which will be reused on building each of the branches of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,Conv2D,  MaxPooling2D, Activation, Flatten, Dropout,Lambda, Dense,Input\n",
    ")\n",
    "import tensorflow as tf\n",
    "\n",
    "class UtkMultiOutputModel():\n",
    "    \"\"\"\n",
    "    Used to generate our multi-output model. This CNN contains three branches, one for age, other for \n",
    "    sex and another for race. Each branch contains a sequence of Convolutional Layers that is defined\n",
    "    on the make_default_hidden_layers method.\n",
    "    \"\"\"\n",
    "    def make_default_hidden_layers(self, inputs):\n",
    "        \"\"\"\n",
    "        Used to generate a default set of hidden layers. The structure used in this network is defined as:\n",
    "        \n",
    "        Conv2D -> BatchNormalization -> Pooling -> Dropout\n",
    "        \"\"\"\n",
    "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_race_branch(self, inputs, num_races):\n",
    "        \"\"\"\n",
    "        Used to build the race branch of our face recognition network.\n",
    "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
    "        followed by the Dense output layer.\n",
    "        \"\"\"\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_races)(x)\n",
    "        x = Activation(\"softmax\", name=\"race_output\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_gender_branch(self, inputs, num_genders=2):\n",
    "        \"\"\"\n",
    "        Used to build the gender branch of our face recognition network.\n",
    "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
    "        followed by the Dense output layer.\n",
    "        \"\"\"\n",
    "        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
    "\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_genders)(x)\n",
    "        x = Activation(\"sigmoid\", name=\"gender_output\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_age_branch(self, inputs):   \n",
    "        \"\"\"\n",
    "        Used to build the age branch of our face recognition network.\n",
    "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
    "        followed by the Dense output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation(\"linear\", name=\"age_output\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def assemble_full_model(self, width, height, num_races):\n",
    "        \"\"\"\n",
    "        Used to assemble our multi-output model CNN.\n",
    "        \"\"\"\n",
    "        input_shape = (height, width, 3)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        age_branch = self.build_age_branch(inputs)\n",
    "        race_branch = self.build_race_branch(inputs, num_races)\n",
    "        gender_branch = self.build_gender_branch(inputs)\n",
    "\n",
    "        model = Model(inputs=inputs,\n",
    "                     outputs = [age_branch, race_branch, gender_branch],\n",
    "                     name=\"face_net\")\n",
    "\n",
    "        return model\n",
    "    \n",
    "model = UtkMultiOutputModel().assemble_full_model(IM_WIDTH, IM_HEIGHT, num_races=len(dataset_dict['race_alias']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look into our model structure and summary, to have a better understanding of what we are building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "Now it's time to train our multi-output model, once we already have everything done. But before doing it, we need to compile our model. For this step, let's use a learning rate of 0.0004 and an Adam optimizer. We will also use custom loss weights for each feature and a custom loss function for each feature.\n",
    "\n",
    "When building our optimizer, let's use a decay based on the learning rate divided by the number of epochs, so we will slowly be decreasing our learning rate over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "init_lr = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "opt = Adam(learning_rate=init_lr, decay=init_lr / epochs)\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss={\n",
    "                  'age_output': 'mse', \n",
    "                  'race_output': 'categorical_crossentropy', \n",
    "                  'gender_output': 'binary_crossentropy'},\n",
    "              loss_weights={\n",
    "                  'age_output': 4., \n",
    "                  'race_output': 1.5, \n",
    "                  'gender_output': 0.1},\n",
    "              metrics={\n",
    "                  'age_output': 'mae', \n",
    "                  'race_output': 'accuracy',\n",
    "                  'gender_output': 'accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's train our model with a batch size of 32 for both valid and train sets. We will be using a ModelCheckpoint callback in order to save the model on disk at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/362 [==============================] - ETA: 0s - loss: 13.6227 - age_output_loss: 2.7205 - race_output_loss: 1.7831 - gender_output_loss: 0.6621 - age_output_mae: 1.2777 - race_output_accuracy: 0.4242 - gender_output_accuracy: 0.7280INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "362/362 [==============================] - 2615s 7s/step - loss: 13.6227 - age_output_loss: 2.7205 - race_output_loss: 1.7831 - gender_output_loss: 0.6621 - age_output_mae: 1.2777 - race_output_accuracy: 0.4242 - gender_output_accuracy: 0.7280 - val_loss: 17.3687 - val_age_output_loss: 2.3056 - val_race_output_loss: 5.3240 - val_gender_output_loss: 1.6039 - val_age_output_mae: 1.2353 - val_race_output_accuracy: 0.3817 - val_gender_output_accuracy: 0.4899\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 32\n",
    "valid_batch_size = 32\n",
    "train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size)\n",
    "valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size)\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch=len(train_idx)//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=len(valid_idx)//valid_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our losses and accuracy curves for each feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UtkMultiOutputModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11380/4252392768.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUtkMultiOutputModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massemble_full_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIM_WIDTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIM_HEIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_races\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'race_alias'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'~/model_checkpoint'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UtkMultiOutputModel' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = UtkMultiOutputModel().assemble_full_model(IM_WIDTH, IM_HEIGHT, num_races=len(dataset_dict['race_alias']))\n",
    "tf.saved_model.load('~/model_checkpoint')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['race_output_acc'],\n",
    "                    name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_race_output_acc'],\n",
    "                    name='Valid'))\n",
    "\n",
    "\n",
    "fig.update_layout(height=450, \n",
    "                  width=600,\n",
    "                  title='Accuracy for race feature',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Accuracy')\n",
    "\n",
    "fig.write_html('acc_race.html', include_plotlyjs='cdn')\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scattergl(\n",
    "                    y=history.history['loss'],\n",
    "                    name='Train'))\n",
    "\n",
    "fig.add_trace(go.Scattergl(\n",
    "                    y=history.history['val_loss'],\n",
    "                    name='Valid'))\n",
    "\n",
    "\n",
    "fig.update_layout(height=450, \n",
    "                  width=600,\n",
    "                  title='Overall loss',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss')\n",
    "\n",
    "fig.write_html('overall_loss.html', include_plotlyjs='cdn')\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cr_race = classification_report(race_true, race_pred, target_names=dataset_dict['race_alias'].keys())\n",
    "print(cr_race)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "n = 16\n",
    "random_indices = np.random.permutation(n)\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(n / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 17))\n",
    "for i, img_idx in enumerate(random_indices):\n",
    "    ax = axes.flat[i]\n",
    "    ax.imshow(images[img_idx])\n",
    "    \n",
    "    cur_age_pred = age_pred[img_idx]\n",
    "    cur_age_true = age_true[img_idx]\n",
    "    \n",
    "    cur_gender_pred = gender_pred[img_idx]\n",
    "    cur_gender_true = gender_true[img_idx]\n",
    "    \n",
    "    cur_race_pred = race_pred[img_idx]\n",
    "    cur_race_true = race_true[img_idx]\n",
    "    \n",
    "    age_threshold = 10\n",
    "    if cur_gender_pred == cur_gender_true and cur_race_pred == cur_race_true and abs(cur_age_pred - cur_age_true) <= age_threshold:\n",
    "        ax.xaxis.label.set_color('green')\n",
    "    elif cur_gender_pred != cur_gender_true and cur_race_pred != cur_race_true and abs(cur_age_pred - cur_age_true) > age_threshold:\n",
    "        ax.xaxis.label.set_color('red')\n",
    "    \n",
    "    ax.set_xlabel('a: {}, g: {}, r: {}'.format(int(age_pred[img_idx]),\n",
    "                            dataset_dict['gender_id'][gender_pred[img_idx]],\n",
    "                               dataset_dict['race_id'][race_pred[img_idx]]))\n",
    "    \n",
    "    ax.set_title('a: {}, g: {}, r: {}'.format(int(age_true[img_idx]),\n",
    "                            dataset_dict['gender_id'][gender_true[img_idx]],\n",
    "                               dataset_dict['race_id'][race_true[img_idx]]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('preds.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTK Face Dataset: http://aicip.eecs.utk.edu/wiki/UTKFace\n",
    "\n",
    "Keras Multi-output documentation: https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "SanjayaSubedi post on multi-output model: https://sanjayasubedi.com.np/deeplearning/multioutput-keras/\n",
    "\n",
    "PyImageSearch post on FashionNet: https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/\n",
    "\n",
    "Plotly: https://plot.ly/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
